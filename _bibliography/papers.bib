---
---

@article{puig_boosting_2024,
	bibtex_show = {true},
	title = {Boosting {Cardiac} {Color} {Doppler} {Frame} {Rates} with {Deep} {Learning}},
	volume = {71},
	issn = {1525-8955},
	url = {https://ieeexplore.ieee.org/document/10587281},
	doi = {10.1109/TUFFC.2024.3424549},
	abstract = {Color Doppler echocardiography enables visualization of blood flow within the heart. However, the limited frame rate impedes the quantitative assessment of blood velocity throughout the cardiac cycle, thereby compromising a comprehensive analysis of ventricular filling. Concurrently, deep learning is demonstrating promising outcomes in post-processing of echocardiographic data for various applications. This work explores the use of deep learning models for intracardiac Doppler velocity estimation from a reduced number of filtered I/Q signals. We used a supervised learning approach by simulating patient-based cardiac color Doppler acquisitions and proposed data augmentation strategies to enlarge the training dataset. We implemented architectures based on convolutional neural networks. In particular, we focused on comparing the U-Net model and the recent ConvNeXt models, alongside assessing real-valued versus complex-valued representations. We found that both models outperformed the state-of-the-art autocorrelator method, effectively mitigating aliasing and noise. We did not observe significant differences between the use of real and complex data. Finally, we validated the models on in vitro and in vivo experiments. All models produced quantitatively comparable results to the baseline and were more robust to noise. ConvNeXt emerged as the sole model to achieve high-quality results on in vivo aliased samples. These results demonstrate the interest of supervised deep learning methods for Doppler velocity estimation from a reduced number of acquisitions.},
	number = {11},
	journal = {IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
	author = {Puig, J. and Friboulet, D. and Ling, H. J. and Varray, F. and Mougharbel, M. and Porée, J. and Provost, J. and Garcia, D. and Millioz, F.},
	month = nov,
	year = {2024},
	pages = {1540-1551},
	arxiv = {2404.00067},
	preview = {boosting.jpg},
	selected = {true},
	dimensions={true},
	google_scholar_id={Tyk-4Ss8FVUC},
}

@article{ling_physics-guided_2024,
	bibtex_show = {true},
	title = {Physics-{Guided} {Neural} {Networks} for {Intraventricular} {Vector} {Flow} {Mapping}},
	volume = {71},
	issn = {1525-8955},
	url = {https://ieeexplore.ieee.org/document/10552308},
	doi = {10.1109/TUFFC.2024.3411718},
	abstract = {Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify color Doppler in cardiac imaging. In this study, we propose novel alternatives to the traditional iVFM optimization scheme by utilizing physics-informed neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach. When evaluated on simulated color Doppler images derived from a patient-specific computational fluid dynamics model and in vivo Doppler acquisitions, both approaches demonstrate comparable reconstruction performance to the original iVFM algorithm. The efficiency of PINNs is boosted through dual-stage optimization and pre-optimized weights. On the other hand, the nnU-Net method excels in generalizability and real-time capabilities. Notably, nnU-Net shows superior robustness on sparse and truncated Doppler data while maintaining independence from explicit boundary conditions. Overall, our results highlight the effectiveness of these methods in reconstructing intraventricular vector blood flow. The study also suggests potential applications of PINNs in ultrafast color Doppler imaging and the incorporation of fluid dynamics equations to derive biomarkers for cardiovascular diseases based on blood flow.},
	number = {11},
	journal = {IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
	author = {Ling, H. J. and Bru, S. and Puig, J. and Vixège, F. and Mendez, S. and Nicoud, F. and Courand, P.-Y. and Bernard*, O. and Garcia*, D.},
	month = nov,
	year = {2024},
  	pages = {1377-1388},
	arxiv = {2403.13040},
	preview = {nnunet.gif},
	selected = {true},
	dimensions={true},
	google_scholar_id={3fE2CSJIrl8C},
	video = {https://youtube.com/playlist?list=PLTdwzEviyUHmZrmMyOV4IBk_i4aw7B9lu&si=6IBz8ehdKWKOjaJA},
	annotation = {* O. Bernard and D. Garcia share the last authorship for this work}
}

@article{ling_dealiasing_2023,
	bibtex_show = {true},
	title = {Phase {Unwrapping} of {Color} {Doppler} {Echocardiography} using {Deep} {Learning}},
	volume = {70},
	issn = {0885-3010, 1525-8955},
	url = {https://ieeexplore.ieee.org/document/10163852},
	doi = {10.1109/TUFFC.2023.3289621},
	abstract = {Color Doppler echocardiography is a widely used non-invasive imaging modality that provides real-time information about the intracardiac blood flow. In an apical long-axis view of the left ventricle, color Doppler is subject to phase wrapping, or aliasing, especially during cardiac filling and ejection. When setting up quantitative methods based on color Doppler, it is necessary to correct this wrapping artifact. We developed an unfolded primal-dual network to unwrap (dealias) color Doppler echocardiographic images and compared its effectiveness against two state-of-the-art segmentation approaches based on nnU-Net and transformer models. We trained and evaluated the performance of each method on an in-house dataset and found that the nnU-Net-based method provided the best dealiased results, followed by the primal-dual approach and the transformer-based technique. Noteworthy, the primal-dual network, which had significantly fewer trainable parameters, performed competitively with respect to the other two methods, demonstrating the high potential of deep unfolding methods. Our results suggest that deep learning-based methods can effectively remove aliasing artifacts in color Doppler echocardiographic images, outperforming DeAN, a state-of-the-art semi-automatic technique. Overall, our results show that deep learning-based methods have the potential to effectively preprocess color Doppler images for downstream quantitative analysis.},
	number = {8},
	journal = {IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control},
	author = {Ling, H. J. and Bernard, O. and Ducros, N. and Garcia, D.},
	month = aug,
	year = {2023},
	pages = {810--820},
	arxiv = {2306.13695},
	preview = {dealiasing.jpg},
	dimensions={true},
	google_scholar_id={W7OEmFMy1HYC},
	code = {https://github.com/creatis-myriad/ASCENT},
}

@inproceedings{ling_extraction_2023,
	bibtex_show = {true},
	title = {Extraction of {Volumetric} {Indices} from {Echocardiography}: {Which} {Deep} {Learning} {Solution} for {Clinical} {Use}?},
	isbn = {978-3-031-35302-4},
	shorttitle = {Extraction of {Volumetric} {Indices} from {Echocardiography}},
	url = {https://link.springer.com/chapter/10.1007/978-3-031-35302-4_25},
	doi = {10.1007/978-3-031-35302-4_25},
	abstract = {Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when tested on CAMUS without any fine-tuning, still manage to perform competitively with respect to prior methods. Overall, the experimental results suggest that with sufficient training data, 3D nnU-Net could become the first automated tool to finally meet the standards of an everyday clinical device.},
	language = {en},
	booktitle = {Functional {Imaging} and {Modeling} of the {Heart}},
	author = {Ling, H. J. and Painchaud, N. and Courand, P.-Y. and Jodoin, P.-M. and Garcia, D. and Bernard, O.},
	year = {2023},
	keywords = {deep learning, cardiac segmentation, CNN, temporal segmentation, Ultrasound},
	pages = {245--254},
	arxiv = {2305.01997},
	preview = {0007_A2C.gif},
	dimensions={true},
	google_scholar_id={eQOLeE2rZwMC},
	code = {https://github.com/creatis-myriad/ASCENT},
}

@inproceedings{ling_reaching_2022,
	bibtex_show = {true},
	title = {Reaching intra-observer variability in {2-D} echocardiographic image segmentation with a simple U-Net architecture},
	url = {https://hal.science/hal-03979523/},
	author = {Ling, H. J. and Garcia, D. and Bernard, O.},
	booktitle = {IEEE {International} {Ultrasonics} {Symposium} (IUS)},
	year = {2022},
	html = {https://hal.science/hal-03979523/},
	pdf = {https://www.creatis.insa-lyon.fr/Challenge/camus/files/ieee_ius_2022_ling.pdf},
	poster = {poster_ius_2022.pdf},
	preview = {patient0001_4CH_ED_1.jpg},
	google_scholar_id={5nxA0vEk-isC},
	code = {https://github.com/creatis-myriad/ASCENT},
}
